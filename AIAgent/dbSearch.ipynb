{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806daf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d5114",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_catalog_index(\n",
    "    csv_path: str,\n",
    "    spec_columns: List[str],\n",
    "    embedding_model_name: str = \"all-MiniLM-L6-v2\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Loads a CSV, builds spec-text embeddings, and a FAISS index.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_path: Path to the CSV file.\n",
    "    - spec_columns: List of column names to include in embeddings.\n",
    "    - embedding_model_name: SentenceTransformer model name.\n",
    "\n",
    "    Returns a dict containing:\n",
    "    - df: pandas DataFrame with original data and 'spec_text'\n",
    "    - embed_model: the SentenceTransformer instance\n",
    "    - embeddings: numpy array of normalized embeddings\n",
    "    - index: FAISS IndexFlatIP index over embeddings\n",
    "    - metadata: DataFrame with 'id' and the spec_columns for lookup\n",
    "    \"\"\"\n",
    "    # 1) Load & prepare DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "\n",
    "    # Ensure spec columns exist\n",
    "    missing = [c for c in spec_columns if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing expected columns in CSV: {missing}\")\n",
    "\n",
    "    # Build the combined spec_text column\n",
    "    df[\"spec_text\"] = (\n",
    "        df[spec_columns]\n",
    "        .fillna(\"\")\n",
    "        .astype(str)\n",
    "        .agg(\" \".join, axis=1)\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    )\n",
    "\n",
    "    # 2) Compute embeddings\n",
    "    embed_model = SentenceTransformer(embedding_model_name)\n",
    "    embeddings = embed_model.encode(\n",
    "        df[\"spec_text\"].tolist(),\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    # Normalize for cosine-similarity\n",
    "    faiss.normalize_L2(embeddings)\n",
    "\n",
    "    # 3) Build FAISS index\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # 4) Prepare metadata mapping\n",
    "    metadata = df[[\"id\"] + spec_columns].copy()\n",
    "\n",
    "    return {\n",
    "        \"df\": df,\n",
    "        \"embed_model\": embed_model,\n",
    "        \"embeddings\": embeddings,\n",
    "        \"index\": index,\n",
    "        \"metadata\": metadata\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44540a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def hybrid_search_catalog(\n",
    "    specs: Dict[str, str],\n",
    "    catalog: Dict[str, Any],\n",
    "    top_k: int = 5\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    General hybrid search over any catalog dict.\n",
    "\n",
    "    specs: dict of field→value to match (e.g. {\"series\":\"Predator\",\"cpu_model\":\"i7-12700H\"})\n",
    "    catalog: {\n",
    "      \"df\": DataFrame,\n",
    "      \"embed_model\": SentenceTransformer,\n",
    "      \"embeddings\": np.ndarray,\n",
    "      \"index\": faiss.IndexFlatIP,\n",
    "      \"metadata\": DataFrame  # must have \"id\" plus all possible spec columns\n",
    "    }\n",
    "    \"\"\"\n",
    "    df          = catalog[\"df\"]\n",
    "    embed_model = catalog[\"embed_model\"]\n",
    "    embeddings  = catalog[\"embeddings\"]\n",
    "    index       = catalog[\"index\"]\n",
    "    metadata    = catalog[\"metadata\"]\n",
    "    dim         = embeddings.shape[1]\n",
    "\n",
    "    # 1) keep only non‐empty specs\n",
    "    specs = {k: v for k, v in specs.items() if v}\n",
    "\n",
    "    # 2) pandas filter on exactly those keys\n",
    "    sub = df\n",
    "    for col, val in specs.items():\n",
    "        sub = sub[sub[col].astype(str).str.lower() == val.lower()]\n",
    "\n",
    "    if not sub.empty:\n",
    "        candidate_idx, level = sub.index.to_list(), \"exact\"\n",
    "    else:\n",
    "        # 3) relaxed: drop the last key if more than one\n",
    "        keys = list(specs.keys())\n",
    "        if len(keys) > 1:\n",
    "            relaxed = dict(specs)\n",
    "            relaxed.pop(keys[-1])\n",
    "            sub2 = df\n",
    "            for col, val in relaxed.items():\n",
    "                sub2 = sub2[sub2[col].astype(str).str.lower() == val.lower()]\n",
    "            if not sub2.empty:\n",
    "                candidate_idx, level = sub2.index.to_list(), \"partial\"\n",
    "            else:\n",
    "                candidate_idx, level = list(range(len(df))), \"vector\"\n",
    "        else:\n",
    "            candidate_idx, level = list(range(len(df))), \"vector\"\n",
    "\n",
    "    # 4) build query vector\n",
    "    query_text = \" \".join(specs.values()) if specs else \"\"\n",
    "    q_vec = embed_model.encode([query_text], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_vec)\n",
    "\n",
    "    # 5) FAISS search\n",
    "    if level == \"vector\":\n",
    "        D, I = index.search(q_vec, top_k)\n",
    "    else:\n",
    "        sub_emb = embeddings[np.array(candidate_idx)]\n",
    "        tmp = faiss.IndexFlatIP(dim)\n",
    "        tmp.add(sub_emb)\n",
    "        D, I = tmp.search(q_vec, min(len(candidate_idx), top_k))\n",
    "        I = [[candidate_idx[i] for i in hits] for hits in I]\n",
    "\n",
    "    # 6) format results using only the keys you actually queried\n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        # lookup by row‐index\n",
    "        row = metadata.loc[metadata[\"id\"] == int(df.loc[idx, \"id\"])].iloc[0]\n",
    "        entry = {\n",
    "            \"id\":           int(row[\"id\"]),\n",
    "            \"score\":        float(score),\n",
    "            \"matched_level\": level,\n",
    "        }\n",
    "        # only include the fields you passed in specs\n",
    "        for k in specs.keys():\n",
    "            entry[k] = row[k]\n",
    "        results.append(entry)\n",
    "\n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
